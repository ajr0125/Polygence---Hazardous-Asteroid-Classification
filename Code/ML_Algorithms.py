# -*- coding: utf-8 -*-
"""PolygenceProjectAlgorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18MKUJQ7dswAl0zX8H_qs_Q7na8LbEmpD
"""

#k-fold cross validation function
def k_fold(estim_object,X_train,y_train,cv_value):
  from sklearn.model_selection import cross_val_score
  accuracies = cross_val_score(estimator = estim_object, X = X_train, y = y_train, cv = cv_value)
  print(accuracies)
  print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
  print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

#Import Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from keras.wrappers.scikit_learn import KerasClassifier

#Importing the Data
dataset = pd.read_csv('AsteroidData_Condensed.csv')
X = dataset.iloc[:, 4:-1].values
y = dataset.iloc[:, -1].values
missing = dataset.isnull().sum()
print(missing)

#Taking Care of Missing Data
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X)
X = imputer.transform(X)
#Check how many missing values

#Splitting Dataset into Training and Test Set
from sklearn.model_selection import train_test_split
X_train, X_testdev, y_train, y_testdev = train_test_split(X, y, test_size = 0.2, random_state = 1)
X_test, X_dev, y_test, y_dev = train_test_split(X_testdev, y_testdev, test_size = 0.5, random_state = 1)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train[:, 3:])
X_dev = sc.transform(X_dev[:, 3:])
X_test = sc.transform(X_test[:, 3:])
#Plot values to see if it's normally distributed (most didn't seem to be normally distributed)
plt.hist(X_dev[:,1])

#Training Multiple Linear Regression Model on the Dataset
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
print(regressor.coef_)
print(regressor.intercept_)

#Predicting Output and Calculating Accuracy
y_pred_linreg = regressor.predict(X_dev)
from sklearn.metrics import r2_score
r2_score(y_dev, y_pred_linreg)

#k-fold cross validation
k_fold(regressor, X_train, y_train, 10)

#Training Logistic Regression Model on the Dataset
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)
print(classifier.coef_)

#Making Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred_log = classifier.predict(X_dev)
cm = confusion_matrix(y_dev, y_pred_log)
print(cm)
accuracy_score(y_dev, y_pred_log)

#k-fold cross validation
k_fold(classifier, X_train, y_train, 10)

#Grid Search
param_grid_lr = {
    'max_iter': [20, 50, 100, 200, 500],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'class_weight': ['balanced', None]
}
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
logModel_grid = GridSearchCV(estimator=classifier, param_grid=param_grid_lr, verbose=1, cv=10, n_jobs=-1)
logModel_grid.fit(X_train, y_train)
print(logModel_grid.best_estimator_)

#Accuracy Outputs
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_dev, y_pred_log), ": is the confusion matrix \n")

from sklearn.metrics import accuracy_score
print(accuracy_score(y_dev, y_pred_log), ": is the accuracy score")

#Lasso Regression Model
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
las = Lasso()
parameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}
las_regressor = GridSearchCV(las, parameters, scoring = 'neg_mean_squared_error', cv = 5)

las_regressor.fit(X_train, y_train)
print(las_regressor.best_score_)

#Predicting Output and Calculating Accuracy
y_pred_las = las_regressor.predict(X_dev)
from sklearn.metrics import r2_score
r2_score(y_dev, y_pred_las)

import seaborn as sns
#sns.distplot(y_pred_las)
sns.distplot(y_pred_linreg)

#XGBoost Model
from xgboost import XGBClassifier
classifier_XGB = XGBClassifier()
classifier_XGB.fit(X_train, y_train)

#Test Set Predictions and Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score
y_pred_xgb = classifier_XGB.predict(X_dev)
cm = confusion_matrix(y_dev, y_pred_xgb)
print(cm)
accuracy_score(y_dev, y_pred_xgb)

#k-fold cross validation
k_fold(classifier_XGB, X_train, y_train, 10)

param_grid_xgb = {
        'min_child_weight': [1, 5, 10],
        'gamma': [0, 0.5, 1, 1.5, 2],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'max_depth': [4, 5, 6]
        }
from sklearn.model_selection import GridSearchCV
xgbModel_grid = GridSearchCV(estimator=classifier_XGB, param_grid=param_grid_xgb, verbose=1, cv=10, n_jobs=-1)
xgbModel_grid.fit(X_train, y_train)
print(xgbModel_grid.best_estimator_)

#Accuracy Outputs
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_dev, y_pred_xgb), ": is the confusion matrix \n")

from sklearn.metrics import accuracy_score
print(accuracy_score(y_dev, y_pred_xgb), ": is the accuracy score")

#Random Forest Classification Model
from sklearn.ensemble import RandomForestClassifier
forest_classifier = RandomForestClassifier(n_estimators = 50, random_state = 0)
forest_classifier.fit(X_train, y_train)

#Predicting Test Set Results
y_pred_forest = forest_classifier.predict(X_dev)

#Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_dev, y_pred_forest)
print(cm)
accuracy_score(y_dev, y_pred_forest)

#k-fold cross validation
k_fold(forest_classifier, X_train, y_train, 10)

#Predicting Test Set Results (Random Forest Classifier was the winning model)
y_pred_forest_test = forest_classifier.predict(X_test)

#Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred_forest_test)
print(cm)
accuracy_score(y_test, y_pred_forest_test)

#Random Forest Predictions on the Remaining No's

#Importing the Data
dataset2 = pd.read_csv('AsteroidData_RemainingPostCondense.csv')
X2 = dataset2.iloc[:, 4:-1].values
y2 = dataset2.iloc[:, -1].values
missing = dataset.isnull().sum()
print(missing)

#Taking Care of Missing Data
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X2)
X = imputer.transform(X2)
#Check how many missing values

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X2 = sc.fit_transform(X2[:, 3:])

#Predicting Test Set Results (Random Forest Classifier was the winning model)
y_pred_forest_remaining_nos = forest_classifier.predict(X2)

#Accuracy
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y2, y_pred_forest_remaining_nos)
print(cm)
accuracy_score(y2, y_pred_forest_remaining_nos)

#Grid Search
# Number of trees in random forest
#n_estimators = [5,10,50,100]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(1, 20, num = 5)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
param_grid_forest = {
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
from sklearn.model_selection import GridSearchCV
forestModel_grid = GridSearchCV(estimator=forest_classifier, param_grid=param_grid_forest, verbose=1, cv=10, n_jobs=-1)
forestModel_grid.fit(X_train, y_train)
print(forestModel_grid.best_estimator_)

#Accuracy Outputs
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_dev, y_pred_forest), ": is the confusion matrix \n")

from sklearn.metrics import accuracy_score
print(accuracy_score(y_dev, y_pred_forest), ": is the accuracy score")

#Ridge Regression
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

ridge = Ridge()
parameters = {'alpha':[1e-15, 1e-10, 1e-8, 1e-3, 1e-2, 1, 5, 10, 20, 30, 35, 40, 45, 50, 55, 100]}
ridge_regressor = GridSearchCV(ridge, parameters, scoring = 'neg_mean_squared_error', cv = 5)
ridge_regressor.fit(X_train, y_train)

print(ridge_regressor.best_score_)

#Predicting Output and Calculating Accuracy
y_pred_ridge = ridge_regressor.predict(X_dev)
from sklearn.metrics import r2_score
r2_score(y_dev, y_pred_ridge)

import seaborn as sns
sns.distplot(y_pred_forest)

#Artificial Neural Network

#Building the ANN

#Initializing the ANN
ann = tf.keras.models.Sequential()

#Adding the input layer and the first hidden layer (number of nodes = 2/3 * number of inputs + number of outputs)
ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

#Adding the second hidden layer
ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

#Adding the Dropout Layer
ann.add(tf.keras.layers.Dropout(0.4, input_shape=(9,)))

#Adding the output layer
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

#Training the ANN

#Compiling the ANN
ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#Training ANN on Training Set
ann.fit(X_train, y_train, batch_size = 32, epochs = 10)

#Predicting Test Set Results
y_pred_ann = ann.predict(X_dev)
y_pred_ann = (y_pred_ann > 0.5)

#Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_dev, y_pred_ann)
print(cm)
accuracy_score(y_dev, y_pred_ann)

#Grid Search for Neural Network
def build_ann():
  #Building the ANN

  #Initializing the ANN
  ann = tf.keras.models.Sequential()

  #Adding the input layer and the first hidden layer (number of nodes = 2/3 * number of inputs + number of outputs)
  ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

  #Adding the second hidden layer
  ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

  #Adding the third hidden layer
  ann.add(tf.keras.layers.Dense(units=9, activation='relu'))

  #Adding the output layer
  ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

  #Training the ANN

  #Compiling the ANN
  ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

  return ann

batch_size = [10, 32, 50, 100]
epochs = [10]
#learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
#momentum = [0.0, 0.2, 0.4, 0.6, 0.8]


param_grid_ann = {
               'batch_size': batch_size,
               'epochs': epochs}
from sklearn.model_selection import GridSearchCV
model=KerasClassifier(build_fn=build_ann)
ann_grid = GridSearchCV(estimator=model, param_grid=param_grid_ann, cv=10)
ann_grid = ann_grid.fit(X_train, y_train)
print(ann_grid.best_estimator_)

#Accuracy Outputs
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_dev, y_pred_ann), ": is the confusion matrix \n")

from sklearn.metrics import accuracy_score
print(accuracy_score(y_dev, y_pred_ann), ": is the accuracy score")